{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Supervised Machine Learning Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Satvik Kishore*\n",
    "Netid: sk741\n",
    "\n",
    "*Names of students you worked with on this assignment*: LIST HERE IF APPLICABLE (delete if not)\n",
    "\n",
    "Note: this assignment falls under collaboration Mode 2: Individual Assignment â€“ Collaboration Permitted. Please refer to the syllabus for additional information.\n",
    "\n",
    "Instructions for all assignments can be found [here](https://github.com/kylebradbury/ids705/blob/master/assignments/_Assignment%20Instructions.ipynb), and is also linked to from the [course syllabus](https://kylebradbury.github.io/ids705/index.html).\n",
    "\n",
    "Total points in the assignment add up to 90; an additional 10 points are allocated to presentation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "By successfully completing this assignment you will be able to...\n",
    "- Explain the bias-variance tradeoff of supervised machine learning and the impact of model flexibility on algorithm performance\n",
    "- Perform supervised machine learning training and performance evaluation\n",
    "- Implement a k-nearest neighbors machine learning algorithm from scratch in a style similar to that of popular machine learning tools like `scikit-learn`\n",
    "- Describe how KNN classification works, the method's reliance on distance measurements, and the impact of higher dimensionality on computational speed\n",
    "- Apply regression (linear regression) and classification (KNN) supervised learning techniques to data and evaluate the performance of those methods\n",
    "- Construct simple feature transformations for improving model fit in linear models\n",
    "- Fit a `scikit-learn` supervised learning technique to training data and make predictions using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAC USERS TAKE NOTE:\n",
    "# For clearer plots in Jupyter notebooks on macs, run the following line of code:\n",
    "# %config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual Questions on Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "**[4 points]**\n",
    "For each part below, indicate whether we would generally expect the performance of a flexible statistical learning method to be *better* or *worse* than an inflexible method. Justify your answer.\n",
    "\n",
    "1. The sample size $n$ is extremely large, and the number of predictors $p$ is small.\n",
    "2. The number of predictors $p$ is extremely large, and the number of observations $n$ is small.\n",
    "3. The relationship between the predictors and response is highly non-linear.\n",
    "4. The variance of the error terms, i.e. $\\sigma^2 = Var(\\epsilon)$, is extremely high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "The flexible model will perform *better* because the large number of data, coupled with lower dimensionality will make it difficult for the flexible model to overfit on the data, as errors on a larger set will not allow local overfitting to occur. The larger data will resemble the true population better, while the lesser amount of predictors will allow a greater density of datapoints within the predictor space. A less flexible model will not be able to mimic the data to the degree the flexible model is able to.  \n",
    "2.\n",
    "The flexible model is expected to perform *worse* than the inflexible model. There will be lower density of points and the flebile model is likelier to overfit and generalize poorly. The inflexible model will have the advantage of the constraining model assumptions allowing it to not fit the data too tightly.  \n",
    "3.\n",
    "The flexible model is expected to perform *better* as it will be easier for it to adapt to the nonlinear structure of the data. This answer assumes that the inflexible model's configuration does not assume knowledge of the nature of the non-linearity of the data. Eg: if the data is known to be distributed as a cubic function, an inflexible model constrained to a cubic relationship might perform better than a more non-paramteric flexible model.  \n",
    "4.\n",
    "The inflexible model is expected to perform *better* as it will not try too hard to fit the high variance data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "**[6 points]** For each of the following, (i) explain if each scenario is a classification or regression problem AND why, (ii) indicate whether we are most interested in inference or prediction for that problem AND why, and (iii) provide the sample size $n$ and number of predictors $p$ indicated for each scenario.\n",
    "\n",
    "**(a)** We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.\n",
    "\n",
    "**(b)** We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n",
    "\n",
    "**(c)** We are interesting in predicting the % change in the US dollar in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the dollar, the % change in the US market, the % change in the British market, and the % change in the German market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**  \n",
    "**(a)**  \n",
    "(i) Regression, because salary is a continous variable  \n",
    "(ii) Inference, beacuse we are interested in the factors that influence salary, not salary for previously unknown CEOs  \n",
    "(iii) n = 500, p = 3. (If we one-hot encode \"industry\", p will be higher)  \n",
    "**(b)**  \n",
    "(i) Classification, because Success/Failure is a binary discrete variable  \n",
    "(ii) Prediction, we want to predict the success/failure for a datapoint with an unkown outcome  \n",
    "(iii) n = 20, p = 13  \n",
    "**(c)**  \n",
    "(i) Regression, because the % change is a continous variable  \n",
    "(ii) Prediction, stated in the question  \n",
    "(iii) @@@@@@@@@@@@@@@@@@@@\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "**[6 points] Classification using KNN**. The table below provides a training dataset containing six observations (a.k.a. samples) ($n=6$) each with three predictors (a.k.a. features) ($p=3$), and one qualitative response variable (a.k.a. target).\n",
    "\n",
    "*Table 1. Training dataset with $n=6$ observations in $p=3$ dimensions with a categorical response, $y$*\n",
    "\n",
    "| Obs. | $x_1$ | $x_2$ | $x_3$ | $y$   |\n",
    "|------|-------|-------|-------|-------|\n",
    "| **1**| 0     | 3     | 0     | Red   |\n",
    "| **2**| 2     | 0     | 0     | Red   |\n",
    "| **3**| 0     | 1     | 3     | Red   |\n",
    "| **4**| 0     | 1     | 2     | Blue  |\n",
    "| **5**| -1    | 0     | 1     | Blue  |\n",
    "| **6**| 1     | 1     | 1     | Red   |\n",
    "\n",
    "We want to use the above training dataset to make a prediction, $\\hat{y}$, for an unlabeled test data observation where $x_1=x_2=x_3=0$ using $K$-nearest neighbors. You are given some code below to get you started. *Note: coding is only required for part (a), for (b)-(d) please provide your reasoning based on your answer to part (a)*.\n",
    "\n",
    "**(a)** Compute the Euclidean distance between each observation and the test point, $x_1=x_2=x_3=0$. Present your answer in a table similar in style to Table 1 with observations 1-6 as the row headers.\n",
    "\n",
    "**(b)** What is our prediction, $\\hat{y}$, when $K=1$ for the test point? Why?\n",
    "\n",
    "**(c)** What is our prediction, $\\hat{y}$, when $K=3$ for the test point? Why?\n",
    "\n",
    "**(d)** If the Bayes decision boundary (the optimal decision boundary) in this problem is highly nonlinear, then would we expect the *best* value of $K$ to be large or small? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a)\n",
      "   squared distance to origin  y\n",
      "1                           9  r\n",
      "2                           4  r\n",
      "3                          10  r\n",
      "4                           5  b\n",
      "5                           2  b\n",
      "6                           3  r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.array([[ 0, 3, 0],\n",
    "              [ 2, 0, 0],\n",
    "              [ 0, 1, 3],\n",
    "              [ 0, 1, 2],\n",
    "              [-1, 0, 1],\n",
    "              [ 1, 1, 1]])\n",
    "y = np.array(['r','r','r','b','b','r'])\n",
    "\n",
    "dist = (X**2).sum(axis=1)\n",
    "print(\"(a)\")\n",
    "pd_dist = pd.DataFrame({\"squared distance to origin\": dist, \"y\": y} )\n",
    "pd_dist.index = range(1,7)\n",
    "print(pd_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in below answers, we use squared euclidean distances instead of euclidean distance, as it is easier to compute and represent without change in anwers as sqrt is a monotonic function  \n",
    "(b)  \n",
    "for K = 1, $\\hat{y} = $ blue, because the closest point, pt 5 with squared distance = 2 has $y = $ blue  \n",
    "(c)  \n",
    "for K = 3, $\\hat{y} = $ red, because of the three closest points (2, 5, and 6 with squared distances 4,2,3) have majority (2 out of 3) observations with $y=$ red  \n",
    "(d)  \n",
    "We would expect a large K to perform better. KNN can imitate non-linear boundaries, and smaller K can better form varying boundaries, however, given the very low amount of training data, it would be inadvisable to overfit the training data using a smaller K  @@@@@@@@@@@@@@@\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "**[18 points] Build your own classification algorithm**.\n",
    "\n",
    "**(a)** Build a working version of a binary KNN classifier using the skeleton code below. We'll use the `sklearn` convention that a supervised learning algorithm has the methods `fit` which trains your algorithm (for KNN that means storing the data) and `predict` which identifies the K nearest neighbors and determines the most common class among those K neighbors. *Note: Most classification algorithms typically also have a method `predict_proba` which outputs the confidence score of each prediction, but we will explore that in a later assignment.*\n",
    "\n",
    "**(b)** Load the datasets to be evaluated here. Each includes training features ($\\mathbf{X}$), and test features ($\\mathbf{y}$) for both a low dimensional dataset ($p = 2$ features/predictors) and a higher dimensional dataset ($p = 100$ features/predictors). For each of these datasets there are $n=1000$ observations of each. They can be found in the `data` subfolder in the `assignments` folder on github. Each file is labeled similar to `A2_X_train_low.csv`, which lets you know whether the dataset is of features, $X$, targets, $y$; training or testing; and low or high dimensions.\n",
    "\n",
    "**(c)** Train your classifier on first the low dimensional dataset and then the high dimensional dataset with $k=5$. Evaluate the classification performance on the corresponding test data for each of those trained models. Calculate the time it takes each model to make the predictions and the overall accuracy of those predictions for each corresponding set of test data - state each.\n",
    "\n",
    "**(d)** Compare your implementation's accuracy and computation time to the scikit learn [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) class. How do the results and speed compare to your implementation?\n",
    "\n",
    "**(e)** Some supervised learning algorithms are more computationally intensive during training than testing. What are the drawbacks of the prediction process being slow? In what cases in practice might slow testing (inference) be more problematic than slow training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton code for part (a) to write your own kNN classifier\n",
    "from scipy.stats import mode\n",
    "import time\n",
    "class Knn:\n",
    "# k-Nearest Neighbor class object for classification training and testing\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "                # Save the training data to properties of this class\n",
    "        self.x_train = np.expand_dims(x, axis = 0)\n",
    "        self.y_train = y\n",
    "        return None\n",
    "        \n",
    "    def predict(self, x, k):\n",
    "        y_hat = [] # Variable to store the estimated class label for\n",
    "        dist = np.square(self.x_train - np.expand_dims(x, axis=1)).sum(2) #calculate squared distances\n",
    "        dist = dist.argsort(1) #argsort for each input\n",
    "        dist = dist[:, 0:k] # only select k closest\n",
    "        dist = self.y_train[dist] #replace with y values\n",
    "        y_hat = mode(dist,1).mode[:,0] #get most common from each row\n",
    "        # Return the estimated targets\n",
    "        return list(y_hat)\n",
    "\n",
    "# Metric of overall classification accuracy\n",
    "#  (a more general function, sklearn.metrics.accuracy_score, is also available)\n",
    "def accuracy(y,y_hat):\n",
    "    nvalues = len(y)\n",
    "    accuracy = sum(y == y_hat) / nvalues\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_low = np.genfromtxt('data/A2_X_train_low.csv', delimiter=',')\n",
    "y_train_low = np.genfromtxt('data/A2_y_train_low.csv', delimiter=',')\n",
    "x_train_high = np.genfromtxt('data/A2_X_train_high.csv', delimiter=',')\n",
    "y_train_high = np.genfromtxt('data/A2_y_train_high.csv', delimiter=',')\n",
    "\n",
    "x_test_low = np.genfromtxt('data/A2_X_test_low.csv', delimiter=',')\n",
    "y_test_low = np.genfromtxt('data/A2_y_test_low.csv', delimiter=',')\n",
    "x_test_high = np.genfromtxt('data/A2_X_test_high.csv', delimiter=',')\n",
    "y_test_high = np.genfromtxt('data/A2_y_test_high.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for low model = 0.12\n",
      "accuracy for low model = 0.92\n",
      "time for high model = 0.95\n",
      "accuracy for high model = 0.99\n"
     ]
    }
   ],
   "source": [
    "lowmodel = Knn()\n",
    "lowmodel.fit(x_train_low, y_train_low)\n",
    "low_time = time.time()\n",
    "low_pred = lowmodel.predict(x_test_low, k=5)\n",
    "low_time = time.time() - low_time\n",
    "low_acc = accuracy(y_test_low, low_pred)\n",
    "print(f\"time for low model = {round(low_time,2)}\")\n",
    "print(f\"accuracy for low model = {round(low_acc,2)}\")\n",
    "\n",
    "highmodel = Knn()\n",
    "highmodel.fit(x_train_high, y_train_high)\n",
    "high_time = time.time()\n",
    "high_pred = highmodel.predict(x_test_high, k=5)\n",
    "high_time = time.time() - high_time\n",
    "high_acc = accuracy(y_test_high, high_pred)\n",
    "print(f\"time for high model = {round(high_time,2)}\")\n",
    "print(f\"accuracy for high model = {round(high_acc,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for low model = 0.04\n",
      "accuracy for low model = 0.92\n",
      "time for high model = 0.05\n",
      "accuracy for high model = 0.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "skmodel_low = KNeighborsClassifier(n_neighbors=5)\n",
    "skmodel_low.fit(x_train_low, y_train_low)\n",
    "low_time = time.time()\n",
    "low_pred = skmodel_low.predict(x_test_low)\n",
    "low_time = time.time() - low_time\n",
    "low_acc = accuracy(y_test_low, low_pred)\n",
    "print(f\"time for low model = {round(low_time,2)}\")\n",
    "print(f\"accuracy for low model = {round(low_acc,2)}\")\n",
    "\n",
    "skmodel_high = KNeighborsClassifier(n_neighbors=5)\n",
    "skmodel_high.fit(x_train_high, y_train_high)\n",
    "high_time = time.time()\n",
    "high_pred2 = skmodel_high.predict(x_test_high)\n",
    "high_time = time.time() - high_time\n",
    "high_acc2 = accuracy(y_test_high, high_pred2)\n",
    "print(f\"time for high model = {round(high_time,2)}\")\n",
    "print(f\"accuracy for high model = {round(high_acc2,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the scikit-learn model has the same accuracies as our model, indicating that our model implementation is correct. We also observe that sklearn models predict faster, indicating that our model can be better optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having prediction process being slow can be problematic when the models are deployed for usage. Models that take time to train have the luxury of being trained on better equipment, and this process needs to be done only once. But if a model is slow in predicting, it can be bad because the model might have to make predictions frequently, and could end up existing in weaker machines (like mobile phones), making usage tedious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "**[20 points] Bias-variance tradeoff: exploring the tradeoff with a KNN classifier**. This exercise will illustrate the impact of the bias-variance tradeoff on classifier performance by investigating how model flexibility impacts classifier decision boundaries. For this problem, please us Scikit-learn's KNN implementation rather than your own implementation, as you did at the end of the last question.\n",
    "\n",
    "**(a)** Create a synthetic dataset (with both features and targets). Use the [`make_moons`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html#sklearn.datasets.make_moons) module with the parameter `noise=0.35` to generate 1000 random samples.\n",
    "\n",
    "**(b)** Visualize your data: scatterplot your random samples with each class in a different color.\n",
    "\n",
    "**(c)** Create 3 different data subsets by selecting 100 of the 1000 data points at random three times (with replacement). For each of these 100-sample datasets, fit three separate k-Nearest Neighbor classifiers with: $k = \\{1, 25, 50\\}$. This will result in 9 combinations (3 datasets, each with 3 trained classifiers).\n",
    "\n",
    "**(d)** For each combination of dataset and trained classifier plot the decision boundary (similar in style to Figure 2.15 from *Introduction to Statistical Learning*). This should form a 3-by-3 grid. Each column should represent a different value of $k$ and each row should represent a different dataset. \n",
    "\n",
    "**(e)** What do you notice about the difference between the rows and the columns. Which decision boundaries appear to best separate the two classes of data? Which decision boundaries vary the most as the data change?\n",
    "\n",
    "**(f)** Explain the bias-variance tradeoff using the example of the plots you made in this exercise and its implications for training supervised machine learning algorithms.\n",
    "\n",
    "Notes and tips for plotting decision boundaries (as in part d):\n",
    "- *Resource for plotting decision boundaries with meshgrid and contour: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html*\n",
    "- If you would like to change the colors of the background, and do not like any of the existing cmap available in matplotlib, you can make your own cmap using the 2 sets of rgb values. Sample code (replace r, g, b with respective rgb values):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "x,y = make_moons(noise=0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.scatter(x[:,0], x[:,1])\n",
    "ax.set_xlabel(\"x0\")\n",
    "ax.set_ylabel(\"x1\")\n",
    "#fig.show()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8q/ss44h_cj7vv7xlb3s9qkktr40000gn/T/ipykernel_1105/1270424544.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSegmentedColormap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnewcmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSegmentedColormap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "newcmp = LinearSegmentedColormap.from_list(\"new\", [(r/255, g/255, b/255), (r/255, g/255, b/255)], N=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "**[18 points] Bias-variance trade-off II: Quantifying the tradeoff**. This exercise explores the impact of the bias-variance tradeoff on classifier performance by looking at the performance on both training and test data.\n",
    "\n",
    "Here, the value of $k$ determines how flexible our model is.\n",
    "\n",
    "**(a)** Using the function created earlier to generate random samples (using the `make_moons` function setting the `noise` parameter to 0.35), create a new set of 1000 random samples, and call this dataset your test set and the previously created dataset your training set.\n",
    "\n",
    "**(b)** Train a kNN classifier on your training set for $k = 1,2,...500$. Apply each of these trained classifiers to both your training dataset and your test dataset and plot the classification error (fraction of incorrect predictions).\n",
    "\n",
    "**(c)** What trend do you see in the results?\n",
    "\n",
    "**(d)** What values of $k$ represent high bias and which represent high variance?\n",
    "\n",
    "**(e)** What is the optimal value of $k$ and why?\n",
    "\n",
    "**(f)** In KNN classifiers, the value of k controls the flexibility of the model - what controls the flexibility of other models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\n",
    "**[18 points] Linear regression and nonlinear transformations**. Linear regression can be used to model nonlinear relationships when feature variables are properly transformed to represent the nonlinearities in the data. In this exercise, you're given training and test data contained in files \"A2_Q7_train.csv\" and \"A2_Q7_test.csv\" in the \"data\" folder for this assignment. Your goal is to develop a regression algorithm from the training data that performs well on the test data.\n",
    "\n",
    "*Hint: Use the scikit learn [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) module.*\n",
    "\n",
    "**(a)** Create a scatter plot of your training data.\n",
    "\n",
    "**(b)** Estimate a linear regression model ($y = a_0 + a_1 x$) for the training data and calculate both the $R^2$ value and mean square error for the fit of that model for the training data. Also provide the equation representing the estimated model (e.g. $y = a_0 + a_1 x$, but with the estimated coefficients inserted. Consider this your baseline model against which you will compare other model options. *Evaluating performance on the training data is not a measure of how well this model would generalize to unseen data. We will evaluate performance on the test data once we see our models fit the training data decently well.*  \n",
    "\n",
    "**(c)** If features can be nonlinearly transformed, a linear model may incorporate those non-linear feature transformation relationships in the training process. From looking at the scatter plot of the training data, choose a transformation of the predictor variable, $x$ that may make sense for these data. This will be a multiple regression model of the form $y = a_0 + a_1 z_1 + a_2 z_2 + \\ldots + a_n z_n$. Here $z_i$ could be any transformations of x - perhaps it's $\\frac{1}{x}$, $log(x)$, $sin(x)$, $x^k$ (where $k$ is any power of your choosing). Provide the estimated equation for this multiple regression model (e.g. if you chose your predictors to be $z_1 = x$ and $z_2 = log(x)$, your model would be of the form $y = a_0 + a_1 x + a_2 log(x)$. Also provide the $R^2$ and mean square error of the fit for the training data.\n",
    "\n",
    "**(d)** Visualize the model fit to the training data. Using both of the models you created in parts (b) and (c), plot the original data (as a scatter plot) AND the curves representing your models (each as a separate curve) from (b) and (c).\n",
    "\n",
    "**(e)** Now its time to compare your models and evaluate the generalization performance on held out test data. Using the models above from (b) an (c), apply them to the test data and estimate the $R^2$ and mean square error of the test dataset.\n",
    "\n",
    "**(f)** Which models perform better on the training data, and which on the test data? Why?\n",
    "\n",
    "**(g)** Imagine that the test data were significantly different from the training dataset. How might this affect the predictive capability of your model? How would the accuracy of generalization performance be impacted? Why?\n",
    "\n",
    "*To help get you started - here's some code to help you load in the data for this exercise (you'll just need to update the path)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = './data/'\n",
    "train = pd.read_csv(path + 'A2_Q7_train.csv')\n",
    "test = pd.read_csv(path + 'A2_Q7_test.csv')\n",
    "\n",
    "x_train = train.x.values\n",
    "y_train = train.y.values\n",
    "\n",
    "x_test = test.x.values\n",
    "y_test = test.y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
